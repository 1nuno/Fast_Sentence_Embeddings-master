{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute & Compare Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autotime\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "from fse.models import Sentence2Vec\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from re import sub\n",
    "import pandas as pd\n",
    "from wordfreq import get_frequency_dict\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a pre-trained embedding that is compatible with any of the Gensim models and load it. For example, the original Word2Vec embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load if not on disk\n",
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz -P data/\n",
    "!gunzip data/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word2vec model\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/reddit/\"\n",
    "\n",
    "p = pathlib.Path(data_path)\n",
    "\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(\"Directory does not exist.\")\n",
    "\n",
    "file_list=[]\n",
    "for f in p.iterdir():\n",
    "    if f.is_file():\n",
    "        file_list.append(f)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "        \n",
    "for i, f in enumerate(file_list):\n",
    "    df_tmp = pd.read_csv(f)\n",
    "    df_tmp[\"label\"] = i\n",
    "    df_tmp = df_tmp[[\"title\", \"label\"]]\n",
    "    data = pd.concat([data, df_tmp])\n",
    "    \n",
    "min_data = np.min(np.unique(data.label.values, return_counts=True)[1])\n",
    "labels = np.unique(data.label.values)\n",
    "\n",
    "data_balanced = pd.DataFrame()\n",
    "\n",
    "for i in labels:\n",
    "    data_balanced = pd.concat([data_balanced, data[data[\"label\"] == i].sample(n=min_data, random_state=42)])\n",
    "    \n",
    "data_balanced = data_balanced.sample(frac=1)\n",
    "y = np.array(data_balanced.label.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(sentence):\n",
    "    return [sub(\"[^a-zA-Z]\", \"\", w.lower()) for w in sentence.split()] \n",
    "\n",
    "data_balanced[\"title_processed\"] = (data_balanced['title'].apply(normalize_text))\n",
    "\n",
    "corpus = data_balanced[\"title_processed\"].values.tolist()\n",
    "labels = data_balanced.label.values.tolist()\n",
    "\n",
    "corpus = [[w for w in s if w in model.vocab] for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit_transform([\" \".join(s) for s in corpus])\n",
    "x_tfidf = TfidfTransformer(use_idf=True).fit_transform(x_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Sentence2Vec(model, alpha=0, components=0, no_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cbow = cbow_model.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sif_model = Sentence2Vec(model, alpha=1e-3, components=1, no_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sif = sif_model.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "mds = dict()\n",
    "\n",
    "mds[\"BOW\"] = x_bow\n",
    "mds[\"TFIDF\"] = x_tfidf\n",
    "mds[\"CBOW\"] = x_cbow\n",
    "mds[\"SIF\"] = x_sif\n",
    "\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "\n",
    "p = pathlib.Path(\"excel\")\n",
    "p.mkdir(exist_ok=True)\n",
    "\n",
    "with pd.ExcelWriter(\"excel/pcomp_\"+date_time+\".xlsx\") as writer:\n",
    "    for k in mds.keys():\n",
    "        x_train, x_test, y_train, y_test = train_test_split(mds[k], labels, test_size=0.5, random_state=42)\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "\n",
    "        df = pd.DataFrame(metrics.classification_report(y_test, y_pred, output_dict=True)).T\n",
    "        df.to_excel(writer, sheet_name=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STS Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the STS Benchmark Dataset from: http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark.\n",
    "Some of the lines may be skipped due to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/stsbenchmark/sts-dev.csv\"\n",
    "\n",
    "p = pathlib.Path(file_path)\n",
    "\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(\"Directory does not exist.\")\n",
    "\n",
    "sts_data = pd.read_csv(file_path, sep=\"\\t\", error_bad_lines=False, header=None)\n",
    "sts_data = sts_data[[5,6,4]]\n",
    "sts_data.columns = [\"A\", \"B\", \"sim\"]\n",
    "sts_data.dropna(inplace=True)\n",
    "sts_data.A = (sts_data.A.apply(normalize_text))\n",
    "sts_data.B = (sts_data.B.apply(normalize_text))\n",
    "\n",
    "sents_a = sts_data.A.values.tolist()\n",
    "sents_b = sts_data.B.values.tolist()\n",
    "assert len(sents_a) == len(sents_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_vecs_a = cbow_model.train(sents_a)\n",
    "cbow_vecs_b = cbow_model.train(sents_b)\n",
    "cbow_model.normalize(cbow_vecs_a)\n",
    "cbow_model.normalize(cbow_vecs_b)\n",
    "\n",
    "sif_vecs_a = sif_model.train(sents_a)\n",
    "sif_vecs_b = sif_model.train(sents_b)\n",
    "sif_model.normalize(sif_vecs_a)\n",
    "sif_model.normalize(sif_vecs_b)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"STS\"] = sts_data.sim\n",
    "\n",
    "def pearson_correlation(mat_a, mat_b):\n",
    "    assert mat_a.shape == mat_b.shape\n",
    "    results = []\n",
    "    for i in range(len(mat_a)):\n",
    "        results.append(mat_a[i].dot(mat_b[i]))\n",
    "    return results\n",
    "\n",
    "results[\"CBOW\"] = pearson_correlation(cbow_vecs_a, cbow_vecs_b)\n",
    "results[\"SIF\"] = pearson_correlation(sif_vecs_a, sif_vecs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.corr()\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "results.to_excel(\"excel/STScomp_\"+date_time+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
