{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute & Compare Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 2.23 ms\n"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.models import KeyedVectors\n",
    "from fse.models import Sentence2Vec\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from re import sub\n",
    "import pandas as pd\n",
    "from wordfreq import get_frequency_dict\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a pre-trained embedding that is compatible with any of the Gensim models and load it. For example, the original Word2Vec embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load if not on disk\n",
    "!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz -P data/\n",
    "!gunzip data/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 17:09:09,434 : INFO : loading projection weights from data/GoogleNews-vectors-negative300.bin\n",
      "2019-06-07 17:09:09,435 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-06-07 17:11:03,080 : INFO : loaded (3000000, 300) matrix from data/GoogleNews-vectors-negative300.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained word2vec model\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 95.6 ms\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/reddit/\"\n",
    "\n",
    "p = pathlib.Path(data_path)\n",
    "\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(\"Directory does not exist.\")\n",
    "\n",
    "file_list=[]\n",
    "for f in p.iterdir():\n",
    "    if f.is_file():\n",
    "        file_list.append(f)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "        \n",
    "for i, f in enumerate(file_list):\n",
    "    df_tmp = pd.read_csv(f)\n",
    "    df_tmp[\"label\"] = i\n",
    "    df_tmp = df_tmp[[\"title\", \"label\"]]\n",
    "    data = pd.concat([data, df_tmp])\n",
    "    \n",
    "min_data = np.min(np.unique(data.label.values, return_counts=True)[1])\n",
    "labels = np.unique(data.label.values)\n",
    "\n",
    "data_balanced = pd.DataFrame()\n",
    "\n",
    "for i in labels:\n",
    "    data_balanced = pd.concat([data_balanced, data[data[\"label\"] == i].sample(n=min_data, random_state=42)])\n",
    "    \n",
    "data_balanced = data_balanced.sample(frac=1)\n",
    "y = np.array(data_balanced.label.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 58.6 ms\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(sentence):\n",
    "    return [sub(\"[^a-zA-Z]\", \"\", w.lower()) for w in sentence.split()] \n",
    "\n",
    "data_balanced[\"title_processed\"] = (data_balanced['title'].apply(normalize_text))\n",
    "\n",
    "corpus = data_balanced[\"title_processed\"].values.tolist()\n",
    "labels = data_balanced.label.values.tolist()\n",
    "\n",
    "corpus = [[w for w in s if w in model.vocab] for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 53.8 ms\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "x_bow = count_vect.fit_transform([\" \".join(s) for s in corpus])\n",
    "x_tfidf = TfidfTransformer(use_idf=True).fit_transform(x_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 17:18:59,198 : INFO : pre-computing SIF weights\n",
      "2019-06-07 17:18:59,199 : INFO : no frequency mode: using wordfreq for estimation (lang=en)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "cbow_model = Sentence2Vec(model, alpha=0, components=0, no_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 17:19:05,326 : INFO : estimated required memory for 2460 sentences and 300 dimensions: 2 MB (0 GB)\n",
      "2019-06-07 17:19:05,393 : INFO : finished computing sentence embeddings of 2451 effective sentences with 24746 effective words\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67.9 ms\n"
     ]
    }
   ],
   "source": [
    "x_cbow = cbow_model.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 17:19:10,267 : INFO : pre-computing SIF weights\n",
      "2019-06-07 17:19:10,269 : INFO : no frequency mode: using wordfreq for estimation (lang=en)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.97 s\n"
     ]
    }
   ],
   "source": [
    "sif_model = Sentence2Vec(model, alpha=1e-3, components=1, no_frequency=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 17:19:15,852 : INFO : estimated required memory for 2460 sentences and 300 dimensions: 2 MB (0 GB)\n",
      "2019-06-07 17:19:15,888 : INFO : finished computing sentence embeddings of 2451 effective sentences with 24746 effective words\n",
      "2019-06-07 17:19:15,890 : INFO : computing 1 principal components\n",
      "2019-06-07 17:19:15,925 : INFO : removing 1 principal components\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 76.3 ms\n"
     ]
    }
   ],
   "source": [
    "x_sif = sif_model.train(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 946 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "mds = dict()\n",
    "\n",
    "mds[\"BOW\"] = x_bow\n",
    "mds[\"TFIDF\"] = x_tfidf\n",
    "mds[\"CBOW\"] = x_cbow\n",
    "mds[\"SIF\"] = x_sif\n",
    "\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "\n",
    "p = pathlib.Path(\"excel\")\n",
    "p.mkdir(exist_ok=True)\n",
    "\n",
    "with pd.ExcelWriter(\"excel/pcomp_\"+date_time+\".xlsx\") as writer:\n",
    "    for k in mds.keys():\n",
    "        x_train, x_test, y_train, y_test = train_test_split(mds[k], labels, test_size=0.5, random_state=42)\n",
    "        clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "\n",
    "        df = pd.DataFrame(metrics.classification_report(y_test, y_pred, output_dict=True)).T\n",
    "        df.to_excel(writer, sheet_name=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STS Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the STS Benchmark Dataset from: http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark.\n",
    "Some of the lines may be skipped due to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1041: expected 7 fields, saw 8\\nSkipping line 1065: expected 7 fields, saw 8\\nSkipping line 1082: expected 7 fields, saw 8\\nSkipping line 1136: expected 7 fields, saw 8\\nSkipping line 1149: expected 7 fields, saw 8\\nSkipping line 1449: expected 7 fields, saw 9\\nSkipping line 1450: expected 7 fields, saw 9\\nSkipping line 1451: expected 7 fields, saw 9\\nSkipping line 1452: expected 7 fields, saw 9\\nSkipping line 1453: expected 7 fields, saw 9\\nSkipping line 1454: expected 7 fields, saw 9\\nSkipping line 1455: expected 7 fields, saw 9\\nSkipping line 1456: expected 7 fields, saw 9\\nSkipping line 1457: expected 7 fields, saw 9\\nSkipping line 1458: expected 7 fields, saw 9\\nSkipping line 1459: expected 7 fields, saw 9\\nSkipping line 1460: expected 7 fields, saw 9\\nSkipping line 1461: expected 7 fields, saw 9\\nSkipping line 1462: expected 7 fields, saw 9\\nSkipping line 1463: expected 7 fields, saw 9\\nSkipping line 1464: expected 7 fields, saw 9\\nSkipping line 1465: expected 7 fields, saw 9\\nSkipping line 1466: expected 7 fields, saw 9\\nSkipping line 1467: expected 7 fields, saw 9\\nSkipping line 1468: expected 7 fields, saw 9\\nSkipping line 1469: expected 7 fields, saw 9\\nSkipping line 1470: expected 7 fields, saw 9\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 444 ms\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/stsbenchmark/sts-dev.csv\"\n",
    "\n",
    "p = pathlib.Path(file_path)\n",
    "\n",
    "if not p.exists():\n",
    "    raise FileNotFoundError(\"Directory does not exist.\")\n",
    "\n",
    "sts_data = pd.read_csv(file_path, sep=\"\\t\", error_bad_lines=False, header=None)\n",
    "sts_data = sts_data[[5,6,4]]\n",
    "sts_data.columns = [\"A\", \"B\", \"sim\"]\n",
    "sts_data.dropna(inplace=True)\n",
    "sts_data.A = (sts_data.A.apply(normalize_text))\n",
    "sts_data.B = (sts_data.B.apply(normalize_text))\n",
    "\n",
    "sents_a = sts_data.A.values.tolist()\n",
    "sents_b = sts_data.B.values.tolist()\n",
    "assert len(sents_a) == len(sents_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-07 17:19:44,721 : INFO : estimated required memory for 1441 sentences and 300 dimensions: 1 MB (0 GB)\n",
      "2019-06-07 17:19:45,213 : INFO : finished computing sentence embeddings of 1441 effective sentences with 13876 effective words\n",
      "2019-06-07 17:19:45,214 : INFO : estimated required memory for 1441 sentences and 300 dimensions: 1 MB (0 GB)\n",
      "2019-06-07 17:19:45,390 : INFO : finished computing sentence embeddings of 1441 effective sentences with 13681 effective words\n",
      "2019-06-07 17:19:45,391 : INFO : computing L2-norms of sentence embeddings\n",
      "2019-06-07 17:19:45,407 : INFO : computing L2-norms of sentence embeddings\n",
      "2019-06-07 17:19:45,421 : INFO : estimated required memory for 1441 sentences and 300 dimensions: 1 MB (0 GB)\n",
      "2019-06-07 17:19:45,440 : INFO : finished computing sentence embeddings of 1441 effective sentences with 13876 effective words\n",
      "2019-06-07 17:19:45,441 : INFO : computing 1 principal components\n",
      "2019-06-07 17:19:45,453 : INFO : removing 1 principal components\n",
      "2019-06-07 17:19:45,456 : INFO : estimated required memory for 1441 sentences and 300 dimensions: 1 MB (0 GB)\n",
      "2019-06-07 17:19:45,474 : INFO : finished computing sentence embeddings of 1441 effective sentences with 13681 effective words\n",
      "2019-06-07 17:19:45,474 : INFO : computing 1 principal components\n",
      "2019-06-07 17:19:45,485 : INFO : removing 1 principal components\n",
      "2019-06-07 17:19:45,488 : INFO : computing L2-norms of sentence embeddings\n",
      "2019-06-07 17:19:45,504 : INFO : computing L2-norms of sentence embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 808 ms\n"
     ]
    }
   ],
   "source": [
    "cbow_vecs_a = cbow_model.train(sents_a)\n",
    "cbow_vecs_b = cbow_model.train(sents_b)\n",
    "cbow_model.normalize(cbow_vecs_a)\n",
    "cbow_model.normalize(cbow_vecs_b)\n",
    "\n",
    "sif_vecs_a = sif_model.train(sents_a)\n",
    "sif_vecs_b = sif_model.train(sents_b)\n",
    "sif_model.normalize(sif_vecs_a)\n",
    "sif_model.normalize(sif_vecs_b)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "results[\"STS\"] = sts_data.sim\n",
    "\n",
    "def pearson_correlation(mat_a, mat_b):\n",
    "    assert mat_a.shape == mat_b.shape\n",
    "    results = []\n",
    "    for i in range(len(mat_a)):\n",
    "        results.append(mat_a[i].dot(mat_b[i]))\n",
    "    return results\n",
    "\n",
    "results[\"CBOW\"] = pearson_correlation(cbow_vecs_a, cbow_vecs_b)\n",
    "results[\"SIF\"] = pearson_correlation(sif_vecs_a, sif_vecs_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 21.7 ms\n"
     ]
    }
   ],
   "source": [
    "results = results.corr()\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "results.to_excel(\"excel/STScomp_\"+date_time+\".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STS</th>\n",
       "      <th>CBOW</th>\n",
       "      <th>SIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.722721</td>\n",
       "      <td>0.775961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CBOW</th>\n",
       "      <td>0.722721</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.918188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIF</th>\n",
       "      <td>0.775961</td>\n",
       "      <td>0.918188</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           STS      CBOW       SIF\n",
       "STS   1.000000  0.722721  0.775961\n",
       "CBOW  0.722721  1.000000  0.918188\n",
       "SIF   0.775961  0.918188  1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.7 ms\n"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
