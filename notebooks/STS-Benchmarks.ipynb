{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STS Benchmark Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from fse import Average, SIF, uSIF, Vectors, FTVectors\n",
    "from fse import CSplitIndexedList\n",
    "\n",
    "from re import sub\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(threadName)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we require the sentences from the STS benchmark dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= \"../evaluation/sts-test.csv\"\n",
    "similarities, sent_a, sent_b = [], [], []\n",
    "with open(file, \"r\") as f:\n",
    "    for l in f:\n",
    "        line = l.rstrip().split(\"\\t\")\n",
    "        similarities.append(float(line[4]))\n",
    "        sent_a.append(line[5])\n",
    "        sent_b.append(line[6])\n",
    "similarities = np.array(similarities)\n",
    "assert len(similarities) == len(sent_a) == len(sent_b)\n",
    "task_length = len(similarities)\n",
    "\n",
    "for i, obj in enumerate(zip(similarities, sent_a, sent_b)):\n",
    "    print(f\"{i}\\tSim: {obj[0].round(3):.1f}\\t{obj[1]:40s}\\t{obj[2]:40s}\\t\")\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these sentence requires some preparation (i.e. tokenization) to be used in the core input formats.\n",
    "To reproduce the results from the uSIF paper this part is taken from https://github.com/kawine/usif/blob/master/usif.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_punc = re.compile('.*[A-Za-z0-9].*')\n",
    "\n",
    "def prep_token(token):\n",
    "    t = token.lower().strip(\"';.:()\").strip('\"')\n",
    "    t = 'not' if t == \"n't\" else t\n",
    "    return re.split(r'[-]', t)\n",
    "\n",
    "def prep_sentence(sentence):\n",
    "    tokens = []\n",
    "    for token in word_tokenize(sentence):\n",
    "        if not_punc.match(token):\n",
    "            tokens = tokens + prep_token(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the IndexedList object. The IndexedList extends the previously constructed sent_a and sent_b list together. We additionally provide a custom function \"prep_sentence\" which performs all the prepocessing for a single sentence. Therefore we need the extention **CSplitIndexedList**, which provides you the option to provide a custom split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = CSplitIndexedList(sent_a, sent_b, custom_split=prep_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IndexedList returns the core object required for fse to train a sentence embedding: A tuple. This object constists of words (a list of strings) and its corresponding index. The latter is important if multiple cores access the input queue simultaneously. Thus it must be always provided. The index represents the row in the matrix where it can later be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that IndexedList does not convert the sentences inplace but only on calling the __getitem__ method in order to turn the sentence into a tuple. You can access the original sentence using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the models\n",
    "\n",
    "It is required for us to load the models as BaseKeyedVectors or as an BaseWordEmbeddingsModel. For this notebook, I already converted the models to a BaseKeyedVectors instance and saved the corresponding instance on my external harddrive. You have to replicate these steps yourself, because getting all the files can be a bit difficult, as the total filesize is around 15 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, results = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code performs a disk-to-ram training. Passing a path to __wv_mapfile_path__ will store the corresponding word vectors (wv) as a numpy memmap array. This is required, because loading all vectors into ram would would take up a lot of storage unecessary. The wv.vectors file will be replace by its memmap representation, which is why the next models do not require the wv_mapfile_path argument, as they access the same memmap object.\n",
    "\n",
    "The lang_freq=\"en\" induces the frequencies of the words according to the wordfreq package. This functionality allows you to work with pre-trained embeddings which don't come with frequency information. The method overwrites the counts in the glove.wv.vocab class, so that all further models also benefit from this induction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"fasttext-wiki-news-subwords-300\",\n",
    "    \"glove-twitter-100\",\n",
    "    \"glove-twitter-200\",\n",
    "    \"glove-twitter-25\",\n",
    "    \"glove-twitter-50\",\n",
    "    \"glove-wiki-gigaword-100\",\n",
    "    \"glove-wiki-gigaword-200\",\n",
    "    \"glove-wiki-gigaword-300\",\n",
    "    \"glove-wiki-gigaword-50\",\n",
    "    \"paragram-25\",\n",
    "    \"paragram-300-sl999\",\n",
    "    \"paragram-300-ws353\",\n",
    "    \"paranmt-300\",\n",
    "    \"word2vec-google-news-300\",\n",
    "]\n",
    "\n",
    "vectors = {}\n",
    "\n",
    "for name in model_names:\n",
    "    vectors[name] = Vectors.from_pretrained(name, mmap=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = \"fasttext-crawl-subwords-300\"\n",
    "vectors[ft] = FTVectors.from_pretrained(ft, mmap=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"CBOW\" : (Average, {\"lang_freq\":\"en\"}),\n",
    "    \"SIF\" : (SIF, {\"lang_freq\":\"en\", \"components\" : 10}),\n",
    "    \"uSIF\": (uSIF, {\"lang_freq\":\"en\", \"length\": 11}),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "combinations = list(itertools.product(algos.keys(), vectors.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the results for the STS benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally able to compute the STS benchmark values for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to compute the similarities between two sentences.\n",
    "# Task length is the length of the sts dataset.\n",
    "def compute_similarities(task_length, model):\n",
    "    sims = []\n",
    "    for i, j in zip(range(task_length), range(task_length, 2*task_length)):\n",
    "        sims.append(model.sv.similarity(i,j))\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo, name in combinations:\n",
    "    class_obj = algos[algo][0]\n",
    "    args = algos[algo][1]\n",
    "    vec = vectors[name]\n",
    "    m = class_obj(vec, **args)\n",
    "    m.train(sentences)\n",
    "    r = pearsonr(similarities, compute_similarities(task_length, m))[0].round(4) * 100\n",
    "    results[f\"{algo} {name}\"] = r\n",
    "    print(algo, name, f\"{r:2.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Pearson\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you closely study the values above you will find that:\n",
    "- SIF-Glove is almost equivalent to the values reported in http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark\n",
    "- CBOW-Paranmt is a little better than ParaNMT Word Avg. in https://www.aclweb.org/anthology/W18-3012\n",
    "- uSIF-Paranmt is a little worse than ParaNMT+UP in https://www.aclweb.org/anthology/W18-3012\n",
    "- uSIF-Paragram is a little worse than PSL+UP in https://www.aclweb.org/anthology/W18-3012\n",
    "\n",
    "However, I guess those differences might arise due to differences in preprocessing. Too bad we didn't hit 80. If you have any ideas why those values don't match exactly, feel free to contact me anytime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
